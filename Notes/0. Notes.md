# DL projects' notes

# Data 
- Norm => train faster, no local minima => mean0, /std1 => norm between 0,1
- augmentation: using transforms.compose => we ain't creating "more data"
- we're simply creating another copy of the data to generalize the model!
- so, we're using batches of augmentations vs real data, not mix
- RandomCrop: pads img then crop random 28x28 sqr
- ToTensor: img from PIL to Tensor format 
- there's operations done on the row data(PIL) before sending it to tensor.
- normalization is done on either train/test data sets using the "TRAIN" mean,std ![transforms](./image.png)
- ax.imshow(images[i].view(28,28).cpu().numpy()) => pytorch tensor
- val set => taken from training set even tho we calc mean, std, transformed it, yet val set's taken from training(reversing these transformations afterwards), NEVER TOUCH TEST SET => so all research papers are equally in testing terms.
- btw, validation set is somehow soft copy of part of trainning set, just pointer for the same data in memory, if we did any transform => it transforms all training set along! => get deep copy of val to solve this.
- MLP/ANN: can't handle 2D/3D img => we flatten them, next layer squeezes them, the next squeeze more => till we get weighted neurons =  no. classes
- MLP is actually some sort of supervised PCA or specialized autoencoder.


# Reproducability
